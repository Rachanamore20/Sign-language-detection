In this project, we present a novel approach employing
Long Short-Term Memory (LSTM) networks and MediaPipe framework for real-time sign
language detection. Our methodology leverages the sequential nature of sign language
gestures, utilizing LSTM networks to effectively capture temporal dependencies within
the data. By integrating MediaPipe, a versatile framework for real-time perception tasks,
we extract key hand and finger landmarks from video streams, enabling precise gesture
analysis. The synergy between LSTM and MediaPipe facilitates robust and context-aware
sign language interpretation, achieving high accuracy even amidst dynamic hand
movements and varying environmental conditions. Furthermore, we develop an intuitive
user interface, allowing seamless interaction with the system through webcam. Through
rigorous experimentation and evaluation, we demonstrate the efficacy and reliability of
our approach in real-world scenarios. Our sign language detection system holds promise
for fostering greater accessibility and inclusivity, empowering individuals with hearing
impairments to communicate effectively across diverse settings.
